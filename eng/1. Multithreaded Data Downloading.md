# concurrent_data_downloader.py

### üîÑ Multithreaded System for Data Extraction and Aggregation from OpenSearch/Elasticsearch  
A universal high-load ETL tool with support for parallel processing, logging, caching, memory monitoring, and container compatibility.  
This is a wrapper around the `get_dataframe` function ‚Äî part of a private corporate Python library used to interact with OpenSearch.

---

## üìå Purpose

`concurrent_data_downloader.py` is a depersonalized analogue of a high-performance Python module (written by me) for multithreaded data extraction from OpenSearch, offering:

- automatic query generation over time intervals;
- parallel downloading via `ThreadPoolExecutor`;
- intermediate data serialization to `.pkl`;
- final result compilation to `.csv` or a `DataFrame`;
- flexible logging, memory usage control, sorting, and structure configuration.

This tool was actively used in a production ETL system to work with large sharded indexes (e.g., `index-2024.03`, `index-2024.03.12`).

---

## ‚öôÔ∏è Key Features

| Feature                        | Description                                                                                           | Problem It Solves                                       |
|-------------------------------|-------------------------------------------------------------------------------------------------------|----------------------------------------------------------|
| Configuration via `QueryConfigurator` | A class for full query customization: fields, filters, logic, output modes, memory optimization     | Simplifies and standardizes complex query creation       |
| Smart query generation        | Automatically splits time intervals by hours/days, considering `gte/lte`                             | Efficient splitting of large queries into non-overlapping chunks for parallel processing |
| Multithreading                | Parallel download with thread count control                                                           | Significantly speeds up data extraction                  |
| Caching                       | Supports reuse of downloaded `.pkl` files                                                             | Reduces server load and speeds up repeated requests      |
| Progress bar                  | Uses `tqdm` with Airflow-friendly stub alternative                                                    | Tracks progress of data loading                          |
| Memory monitoring             | Logs peak memory usage in a separate thread                                                           | Helps optimize resource consumption                      |
| Flexible logger               | Logging levels: `basic`, `detailed`, `extra`, `full` ‚Äî output to file and/or console                 | Enables detailed auditing and debugging                  |
| Temporary directory cleanup   | Smart logic to delete or preserve intermediate files based on flags                                  | Maintains disk hygiene and controls space usage          |
| DataFrame type downcasting    | Automatically reduces data types in DataFrame (e.g., `int64` ‚Üí `int32`, `object` ‚Üí `category`)        | Lowers memory usage and improves performance             |
| Chunk-based file/DataFrame writing | Writes data in chunks: appends to file or builds DataFrame incrementally                           | Handles large datasets exceeding RAM size, reduces memory peaks |

---

## üß† Architecture & Key Components

### `QueryConfigurator`  
Handles full configuration: fields, event types, filters, error handling, logging, time range logic, and result format.

### `prepare_queries()`  
Splits the requested time range into subintervals and builds subqueries. Supports both daily and monthly index patterns.

### `download_data_parallel()`  
Main data loading engine:
- manages thread execution;
- processes individual tasks via `_process_single_query`;
- logs success/failure;
- compiles results into `.csv` or `DataFrame`.

### Utilities:
- `optimize_dataframe()` ‚Äî downcasts data types to more efficient ones;  
- `aggregate_data_to_variable()` ‚Äî merges all `.pkl` files into the final DataFrame;  
- `validate_fields()` ‚Äî regularly checks field correctness;  
- `write_instance_info()` ‚Äî writes `INFO.md` with request metadata.

---

## Results & Metrics

![alt text](image.png)
- 9.124 billion (9,124,000,000) rows of data were extracted by analysts for research purposes using multithreaded data extraction over the past 80 days as of the date of this report.

![alt text](/src/sleekshot.png)
- Sequential data extraction for the past 180 days would have taken approximately 1,639 hours (68 days). Thanks to the implementation of multithreading, the extraction time was reduced by at least 30 times ‚Äî down to 2.27 days. Moreover, instead of the recommended 30 threads, many used 50‚Äì70 threads, which further accelerated the process and saved even more time.


- Using of optimization sighificantly improves performance:
```
Before optimization:

Peak memory usage: 9003.24 MB
Dataframe memory usage: 5778.92 MB

After optimization:

Peak memory usage: 4028.07 MB
Dataframe memory usage: 3974.63 MB
```