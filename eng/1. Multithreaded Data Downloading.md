# Multithreaded Data Downloader for OpenSearch/Elasticsearch

## Project Overview

A production-grade ETL tool designed for high-volume data extraction from OpenSearch/Elasticsearch clusters. This solution was developed to address critical performance bottlenecks in analytics data pipelines, achieving a **30x performance improvement** while reducing memory consumption by **55%**.

---

## Business Challenge

**Context:** Analytics team required large-scale historical data extraction from sharded OpenSearch indexes for research and reporting.

**Problems identified:**
- Sequential data extraction taking **68+ days** for 180-day periods
- Peak memory usage exceeding **9 GB** causing system instability
- Manual query construction prone to errors and overlapping time ranges
- Lack of progress monitoring and error recovery mechanisms
- No caching mechanism for expensive repeated queries

**Impact:** Significant delays in data delivery, resource inefficiency, and analyst productivity loss.

---

## Solution Architecture

### Core Features

**1. Intelligent Query Sharding**
- Automatic time-based query splitting (hourly/daily intervals)
- Smart handling of monthly (`index-2024.03`) and daily (`index-2024.03.15`) index patterns
- Overlap detection and elimination to prevent duplicate data
- Support for custom time ranges with `gte`/`lte` parameters

**2. Parallel Processing Engine**
- `ThreadPoolExecutor`-based concurrent downloads
- Configurable thread pool (1-70+ threads tested in production)
- Per-query `.pkl` serialization for fault tolerance
- Progress tracking with `tqdm` (supports Airflow-compatible stub mode)

**3. Memory Optimization**
- DataFrame type downcasting (`int64` → `int32`, `object` → `category`)
- Chunked aggregation to handle datasets larger than available RAM
- Incremental file writing (append mode for CSV)
- Automatic garbage collection between operations

**4. Production-Ready Features**
- **Logging:** 4 verbosity levels (`BASIC`, `DETAILED`, `EXTRA`, `FULL`)
- **Error Handling:** Configurable fail-fast vs. continue-on-error behavior
- **Caching:** MD5-based cache directories for query reuse
- **Memory Monitoring:** Separate thread tracking peak consumption
- **Container Support:** Airflow/Docker-compatible execution mode

---

## Technical Implementation

### Technology Stack

| Component | Technology |
|-----------|-----------|
| **Language** | Python 3.8+ |
| **Concurrency** | `concurrent.futures.ThreadPoolExecutor` |
| **Data Processing** | Pandas, NumPy |
| **Monitoring** | psutil, tqdm |
| **Serialization** | Pickle, CSV |
| **Logging** | Custom logger + stdout integration |

### Key Architecture Components

```python
QueryConfigurator
├── Query template management
├── Field validation (regex-based)
├── Time range configuration (gte/lte)
└── Output format control (CSV/DataFrame)

prepare_queries()
├── Index pattern parsing
├── Time interval splitting
├── Query deduplication
└── Cache directory management

download_data_parallel()
├── ThreadPoolExecutor orchestration
├── Memory monitoring thread
├── Progress tracking
└── Result aggregation (streaming or in-memory)

optimize_dataframe()
├── Integer downcasting
├── Float precision reduction
├── Category conversion
└── Memory profiling
```

---

## Results & Impact

### Performance Metrics

```
┌─────────────────────────────────────────────────────────────────────┐
│ METRIC                          │ BEFORE      │ AFTER       │ GAIN  │
├─────────────────────────────────┼─────────────┼─────────────┼───────┤
│ Extraction Time (180 days)      │ 1,639 hours │ 54 hours    │ 30x   │
│                                 │ (68 days)   │ (2.27 days) │       │
├─────────────────────────────────┼─────────────┼─────────────┼───────┤
│ Peak Memory Usage               │ 9,003 MB    │ 4,028 MB    │ 55%   │
├─────────────────────────────────┼─────────────┼─────────────┼───────┤
│ DataFrame Memory                │ 5,779 MB    │ 3,975 MB    │ 31%   │
├─────────────────────────────────┼─────────────┼─────────────┼───────┤
│ Total Rows Extracted (180 days) │ —           │ 9.1 billion │ —     │
└─────────────────────────────────┴─────────────┴─────────────┴───────┘
```

### Business Impact

- **Time Savings:** Analytics team can now get 180 days of data in **2.27 days** vs. 68 days
- **Resource Efficiency:** 55% reduction in memory allows running on smaller instances
- **Scalability:** Successfully processed **9.1+ billion rows** across analytics use cases
- **Reliability:** Automatic error recovery and caching reduced manual intervention by 90%
- **Developer Experience:** Analysts use 50-70 threads (vs. recommended 30) for even faster results

---

## Code Highlights

### Memory Optimization Example

```python
def optimize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Compress DataFrame by downcasting data types to save memory."""
    for col in df.columns:
        col_data = df[col]
        
        # Integer optimization
        if pd.api.types.is_integer_dtype(col_data):
            df[col] = pd.to_numeric(col_data, downcast="integer")
        
        # Float optimization with precision check
        elif pd.api.types.is_float_dtype(col_data):
            for dtype in ["float16", "float32"]:
                downcasted = col_data.astype(dtype)
                if np.allclose(col_data, downcasted, rtol=1e-3, atol=1e-3):
                    df[col] = downcasted
                    break
        
        # Category optimization for low-cardinality columns
        elif pd.api.types.is_object_dtype(col_data):
            if col_data.nunique() / len(col_data) < 0.5:
                df[col] = col_data.astype("category")
    
    return df
```

**Result:** 31% memory reduction in final DataFrame

---

### Chunked Aggregation

```python
def concat_in_chunks(df_iter: Iterator[pd.DataFrame], 
                     chunk_size: int = 10) -> Iterator[pd.DataFrame]:
    """Aggregate DataFrames in chunks to prevent memory spikes."""
    _buffer = []
    for df in df_iter:
        _buffer.append(df)
        if len(_buffer) >= chunk_size:
            yield pd.concat(_buffer, ignore_index=True, copy=False)
            _buffer.clear()
            gc.collect()
    
    if _buffer:
        yield pd.concat(_buffer, ignore_index=True, copy=False)
```

**Benefit:** Enables processing datasets larger than available RAM

---

## Usage Statistics

Real-world production metrics over the last 180 days:

- **Total extractions:** 9.124 billion rows
- **Average speedup:** 30x (theoretical sequential time: 1,639 hours → actual: ~54 hours)
- **Thread usage:** Most analysts use 50-70 threads (exceeding recommended 30) for additional speed gains
- **Cache hit rate:** ~40% of queries reused cached results

---

## Implementation Approach

### Phase 1: Analysis & Design
- Profiled existing sequential extraction process
- Identified bottlenecks: network I/O and single-threaded execution
- Designed thread-safe query sharding algorithm

### Phase 2: Core Development
- Implemented `QueryConfigurator` with validation
- Built parallel download engine with `ThreadPoolExecutor`
- Added caching layer with MD5-based unique directory naming

### Phase 3: Optimization
- Integrated memory monitoring and type optimization
- Implemented chunked aggregation for large datasets
- Added comprehensive logging and error handling

### Phase 4: Production Hardening
- Added container support for Airflow deployment
- Implemented graceful degradation and error recovery
- Created extensive documentation and usage guidelines

---

## Key Learnings

1. **Parallelization ROI:** Thread count vs. speedup is not linear; 30-50 threads showed optimal balance
2. **Memory Management:** Type optimization and chunking can reduce memory usage by >50%
3. **Fault Tolerance:** Caching intermediate results critical for long-running jobs
4. **Monitoring:** Real-time progress tracking essential for user experience in long operations
5. **Production Readiness:** Container compatibility and logging levels make deployment seamless

---

## Related Files

- **Source Code:** [`/common/concurrent_data_downloader.py`](/common/concurrent_data_downloader.py)
- **Visualizations:** 
  - [`/src/image.png`](/src/image.png) — Total rows extracted metrics
  - [`/src/sleekshot.png`](/src/sleekshot.png) — Performance comparison chart

---

## See Also

- [Main Portfolio README](../README.md)
- [Russian Version](../ru/1.%20Многопоточная%20выгрузка%20данных.md)

---

<div align="center">

**This project demonstrates production-level expertise in:**  
`Data Engineering` · `Performance Optimization` · `Concurrent Programming` · `ETL Development`

</div>
