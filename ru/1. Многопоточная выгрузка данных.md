# Многопоточная выгрузка данных из OpenSearch/Elasticsearch

## Обзор проекта

Production-готовый ETL-инструмент, разработанный для высокопроизводительной выгрузки данных из кластеров OpenSearch/Elasticsearch. Решение было создано для устранения критических узких мест в аналитических пайплайнах, достигнув **30-кратного улучшения производительности** при одновременном снижении потребления памяти на **55%**.

---

## Бизнес-задача

**Контекст:** Команде аналитиков требовалась масштабная выгрузка исторических данных из шардированных индексов OpenSearch для исследований и отчётности.

**Выявленные проблемы:**
- Последовательная выгрузка данных занимала **68+ дней** для периода в 180 дней
- Пиковое потребление памяти превышало **9 ГБ**, вызывая нестабильность системы
- Ручное конструирование запросов было подвержено ошибкам и пересекающимся временным диапазонам
- Отсутствие мониторинга прогресса и механизмов восстановления после ошибок
- Отсутствие кэширования для дорогих повторяющихся запросов

**Влияние:** Значительные задержки в поставке данных, неэффективное использование ресурсов и снижение продуктивности аналитиков.

---

## Архитектура решения

### Основные возможности

**1. Интеллектуальное шардирование запросов**
- Автоматическое разбиение запросов по временным интервалам (часовые/дневные)
- Умная обработка месячных (`index-2024.03`) и дневных (`index-2024.03.15`) паттернов индексов
- Обнаружение и устранение пересечений для предотвращения дублирования данных
- Поддержка пользовательских временных диапазонов с параметрами `gte`/`lte`

**2. Движок параллельной обработки**
- Конкурентная загрузка на основе `ThreadPoolExecutor`
- Настраиваемый пул потоков (1-70+ потоков протестировано в продакшене)
- Сериализация каждого запроса в `.pkl` для отказоустойчивости
- Отслеживание прогресса с `tqdm` (поддержка Airflow-совместимого stub-режима)

**3. Оптимизация памяти**
- Даункастинг типов DataFrame (`int64` → `int32`, `object` → `category`)
- Чанковая агрегация для обработки датасетов, превышающих доступную RAM
- Инкрементальная запись файлов (режим append для CSV)
- Автоматическая сборка мусора между операциями

**4. Production-ready функции**
- **Логирование:** 4 уровня детализации (`BASIC`, `DETAILED`, `EXTRA`, `FULL`)
- **Обработка ошибок:** Настраиваемое поведение fail-fast vs. continue-on-error
- **Кэширование:** MD5-based директории кэша для переиспользования запросов
- **Мониторинг памяти:** Отдельный поток для отслеживания пикового потребления
- **Поддержка контейнеров:** Режим выполнения совместимый с Airflow/Docker

---

## Техническая реализация

### Технологический стек

| Компонент | Технология |
|-----------|-----------|
| **Язык** | Python 3.8+ |
| **Конкурентность** | `concurrent.futures.ThreadPoolExecutor` |
| **Обработка данных** | Pandas, NumPy |
| **Мониторинг** | psutil, tqdm |
| **Сериализация** | Pickle, CSV |
| **Логирование** | Custom logger + интеграция со stdout |

### Ключевые архитектурные компоненты

```python
QueryConfigurator
├── Управление шаблонами запросов
├── Валидация полей (на основе regex)
├── Конфигурация временных диапазонов (gte/lte)
└── Контроль формата вывода (CSV/DataFrame)

prepare_queries()
├── Парсинг паттернов индексов
├── Разбиение временных интервалов
├── Дедупликация запросов
└── Управление директориями кэша

download_data_parallel()
├── Оркестрация ThreadPoolExecutor
├── Поток мониторинга памяти
├── Отслеживание прогресса
└── Агрегация результатов (потоковая или в памяти)

optimize_dataframe()
├── Даункастинг целочисленных типов
├── Снижение точности float
├── Преобразование в категории
└── Профилирование памяти
```

---

## Результаты и влияние

### Метрики производительности

```
┌─────────────────────────────────────────────────────────────────────┐
│ МЕТРИКА                         │ ДО          │ ПОСЛЕ       │ ВЫИГРЫШ│
├─────────────────────────────────┼─────────────┼─────────────┼────────┤
│ Время выгрузки (180 дней)       │ 1 639 часов │ 54 часа     │ 30x    │
│                                 │ (68 дней)   │ (2,27 дня)  │        │
├─────────────────────────────────┼─────────────┼─────────────┼────────┤
│ Пиковое потребление памяти      │ 9 003 МБ    │ 4 028 МБ    │ 55%    │
├─────────────────────────────────┼─────────────┼─────────────┼────────┤
│ Память DataFrame                │ 5 779 МБ    │ 3 975 МБ    │ 31%    │
├─────────────────────────────────┼─────────────┼─────────────┼────────┤
│ Всего выгружено строк (180 дней)│ —           │ 9,1 млрд    │ —      │
└─────────────────────────────────┴─────────────┴─────────────┴────────┘
```

### Бизнес-влияние

- **Экономия времени:** Аналитики теперь получают 180 дней данных за **2,27 дня** вместо 68 дней
- **Эффективность ресурсов:** Снижение памяти на 55% позволяет запускать на более дешёвых инстансах
- **Масштабируемость:** Успешно обработано **9,1+ млрд строк** в различных аналитических кейсах
- **Надёжность:** Автоматическое восстановление после ошибок и кэширование сократили ручное вмешательство на 90%
- **Developer Experience:** Аналитики используют 50-70 потоков (против рекомендованных 30) для ещё большей скорости

---

## Ключевые фрагменты кода

### Пример оптимизации памяти

```python
def optimize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Сжимает DataFrame через даункастинг типов данных для экономии памяти."""
    for col in df.columns:
        col_data = df[col]
        
        # Оптимизация целочисленных типов
        if pd.api.types.is_integer_dtype(col_data):
            df[col] = pd.to_numeric(col_data, downcast="integer")
        
        # Оптимизация float с проверкой точности
        elif pd.api.types.is_float_dtype(col_data):
            for dtype in ["float16", "float32"]:
                downcasted = col_data.astype(dtype)
                if np.allclose(col_data, downcasted, rtol=1e-3, atol=1e-3):
                    df[col] = downcasted
                    break
        
        # Оптимизация категорий для столбцов с низкой кардинальностью
        elif pd.api.types.is_object_dtype(col_data):
            if col_data.nunique() / len(col_data) < 0.5:
                df[col] = col_data.astype("category")
    
    return df
```

**Результат:** Снижение памяти финального DataFrame на 31%

---

### Чанковая агрегация

```python
def concat_in_chunks(df_iter: Iterator[pd.DataFrame], 
                     chunk_size: int = 10) -> Iterator[pd.DataFrame]:
    """Агрегирует DataFrames по чанкам для предотвращения пиков памяти."""
    _buffer = []
    for df in df_iter:
        _buffer.append(df)
        if len(_buffer) >= chunk_size:
            yield pd.concat(_buffer, ignore_index=True, copy=False)
            _buffer.clear()
            gc.collect()
    
    if _buffer:
        yield pd.concat(_buffer, ignore_index=True, copy=False)
```

**Преимущество:** Позволяет обрабатывать датасеты, превышающие доступную RAM

---

## Статистика использования

Реальные метрики в продакшене за последние 180 дней:

- **Всего выгружено:** 9,124 млрд строк
- **Среднее ускорение:** в 30 раз (теоретическое последовательное время: 1 639 часов → фактическое: ~54 часа)
- **Использование потоков:** Большинство аналитиков используют 50-70 потоков (превышая рекомендованные 30) для дополнительного ускорения
- **Процент попаданий в кэш:** ~40% запросов переиспользовали кэшированные результаты

---

## Подход к реализации

### Фаза 1: Анализ и проектирование
- Профилирование существующего процесса последовательной выгрузки
- Выявление узких мест: сетевой I/O и однопоточное выполнение
- Проектирование потокобезопасного алгоритма шардирования запросов

### Фаза 2: Основная разработка
- Реализация `QueryConfigurator` с валидацией
- Построение движка параллельной загрузки с `ThreadPoolExecutor`
- Добавление слоя кэширования с уникальными именами директорий на основе MD5

### Фаза 3: Оптимизация
- Интеграция мониторинга памяти и оптимизации типов
- Реализация чанковой агрегации для больших датасетов
- Добавление комплексного логирования и обработки ошибок

### Фаза 4: Подготовка к продакшену
- Добавление поддержки контейнеров для развёртывания в Airflow
- Реализация graceful degradation и восстановления после ошибок
- Создание подробной документации и руководств по использованию

---

## Ключевые выводы

1. **ROI параллелизации:** Зависимость скорости от количества потоков нелинейная; 30-50 потоков показали оптимальный баланс
2. **Управление памятью:** Оптимизация типов и чанкинг могут снизить потребление памяти более чем на 50%
3. **Отказоустойчивость:** Кэширование промежуточных результатов критично для долгоиграющих задач
4. **Мониторинг:** Отслеживание прогресса в реальном времени существенно для пользовательского опыта в длительных операциях
5. **Production-готовность:** Совместимость с контейнерами и уровни логирования делают развёртывание бесшовным

---

## Связанные файлы

- **Исходный код:** [`/common/concurrent_data_downloader.py`](/common/concurrent_data_downloader.py)
- **Визуализации:** 
  - [`/src/image.png`](/src/image.png) — Метрики общего количества выгруженных строк
  - [`/src/sleekshot.png`](/src/sleekshot.png) — График сравнения производительности

---

## См. также

- [Основной README портфолио](../README.md)
- [English Version](../eng/1.%20Multithreaded%20Data%20Downloading.md)

---

<div align="center">

**Этот проект демонстрирует экспертизу production-уровня в:**  
`Инженерия данных` · `Оптимизация производительности` · `Конкурентное программирование` · `Разработка ETL`

</div>
